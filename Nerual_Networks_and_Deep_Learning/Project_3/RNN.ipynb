{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "\n",
    "start_token = 'G'\n",
    "end_token = 'E'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_poems(file_name):\n",
    "    poems = []\n",
    "    with open(file_name, \"r\", encoding='utf-8', ) as f:\n",
    "        for line in f.readlines():\n",
    "            try:\n",
    "                title, content = line.strip().split(':')\n",
    "                content = content.replace(' ', '')\n",
    "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content or \\\n",
    "                                start_token in content or end_token in content:\n",
    "                    continue\n",
    "                if len(content) < 5 or len(content) > 80:\n",
    "                    continue\n",
    "                content = start_token + content + end_token\n",
    "                poems.append(content)\n",
    "            except ValueError as e:\n",
    "                pass\n",
    "    # 按诗的字数排序\n",
    "    poems = sorted(poems, key=lambda line: len(line))\n",
    "    # 统计每个字出现次数\n",
    "    all_words = []\n",
    "    for poem in poems:\n",
    "        all_words += [word for word in poem]  \n",
    "    counter = collections.Counter(all_words)  # 统计词和词频。\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*count_pairs)\n",
    "    words = words[:len(words)] + (' ',)\n",
    "    word_int_map = dict(zip(words, range(len(words))))\n",
    "    poems_vector = [list(map(word_int_map.get, poem)) for poem in poems]\n",
    "    return poems_vector, word_int_map, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rnn_lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model(model, input_data, output_data, vocab_size, rnn_size=128, num_layers=2, batch_size=64,\n",
    "              learning_rate=0.01):\n",
    "    end_points = {}\n",
    "    # 构建RNN基本单元RNNcell\n",
    "    if model == 'rnn':\n",
    "        cell_fun = tf.contrib.rnn.BasicRNNCell\n",
    "    elif model == 'gru':\n",
    "        cell_fun = tf.contrib.rnn.GRUCell\n",
    "    else:\n",
    "        cell_fun = tf.contrib.rnn.BasicLSTMCell\n",
    "    #？？？？？？？？？？？？？？？？？？？？？？\n",
    "    # 每层128个小单元，一共有两层，输出的Ct 和 Ht 要分开放到两个tuple中\n",
    "    # 在下面补全代码 \n",
    "    #################################################\n",
    "    cell = cell_fun(rnn_size, state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    #################################################\n",
    "    # 如果是训练模式，output_data不为None，则初始状态shape为[batch_size * rnn_size]\n",
    "    # 如果是生成模式，output_data为None，则初始状态shape为[1 * rnn_size]\n",
    "    if output_data is not None:\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    else:\n",
    "        initial_state = cell.zero_state(1, tf.float32)\n",
    "\n",
    "    # 构建隐层\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.Variable(tf.random_uniform([vocab_size + 1, rnn_size], -1.0, 1.0),name = 'embedding')\n",
    "        inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    #？？？？？？？？？？？？？？？？？？？？？？？？？？\n",
    "    ####################################################    \n",
    "    outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, dtype=tf.float32)# 填写里面的内容\n",
    "    ######################################################\n",
    "    output = tf.reshape(outputs, [-1, rnn_size])\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([rnn_size, vocab_size + 1]))\n",
    "    bias = tf.Variable(tf.zeros(shape=[vocab_size + 1]))\n",
    "    logits = tf.nn.bias_add(tf.matmul(output, weights), bias=bias) # 一层全连接\n",
    "\n",
    "\n",
    "    if output_data is not None: # 训练模式\n",
    "        labels = tf.one_hot(tf.reshape(output_data, [-1]), depth=vocab_size + 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)  # 优化器用的 adam\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['output'] = output\n",
    "        end_points['train_op'] = train_op\n",
    "        end_points['total_loss'] = total_loss\n",
    "        end_points['loss'] = loss\n",
    "        end_points['last_state'] = last_state\n",
    "    else: # 生成模式\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['last_state'] = last_state\n",
    "        end_points['prediction'] = prediction\n",
    "    return end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    # 处理数据集\n",
    "    poems_vector, word_to_int, vocabularies = process_poems('./poems.txt')\n",
    "    # 生成batch\n",
    "    batches_inputs, batches_outputs = generate_batch(64, poems_vector, word_to_int)\n",
    "\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    output_targets = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    # 构建模型\n",
    "    end_points = rnn_model(model='lstm', input_data=input_data, output_data=output_targets, vocab_size=len(\n",
    "        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=0.01)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        for epoch in range(50):\n",
    "            n = 0\n",
    "            n_chunk = len(poems_vector) // batch_size\n",
    "            for batch in range(n_chunk):\n",
    "                loss, _, _ = sess.run([\n",
    "                    end_points['total_loss'],\n",
    "                    end_points['last_state'],\n",
    "                    end_points['train_op']\n",
    "                ], feed_dict={input_data: batches_inputs[n], output_targets: batches_outputs[n]})\n",
    "                n += 1\n",
    "                print('[INFO] Epoch: %d , batch: %d , training loss: %.6f' % (epoch, batch, loss))\n",
    "        saver.save(sess, './poem_generator')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成 诗歌部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_poem(begin_word):\n",
    "    batch_size = 1\n",
    "    poems_vector, word_int_map, vocabularies = process_poems('./poems.txt')\n",
    "\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "\n",
    "    end_points = rnn_model(model='lstm', input_data=input_data, output_data=None, vocab_size=len(\n",
    "        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=0.01)\n",
    "    # 如果指定开始的字\n",
    "    if begin_word:\n",
    "        word = begin_word\n",
    "    else:\n",
    "        word = to_word(predict, vocabularies)\n",
    "        \n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        saver.restore(sess, './poem_generator')# 恢复之前训练好的模型 \n",
    "        poem = ''\n",
    "        #???????????????????????????????????????\n",
    "        # 下面部分代码主要功能是根据指定的开始字符来生成诗歌\n",
    "        #########################################\n",
    "        cur_state = sess.run(end_points['last_state'], feed_dict={\n",
    "            input_data: np.array([[word_int_map[start_token]]])\n",
    "        })\n",
    "        while word != end_token:\n",
    "            poem += word\n",
    "            index = np.array([[word_int_map[word]]])\n",
    "            probs, cur_state = sess.run([\n",
    "                end_points['prediction'],\n",
    "                end_points['last_state']\n",
    "            ], feed_dict={\n",
    "                input_data: index,\n",
    "                end_points['initial_state']: cur_state\n",
    "            })\n",
    "            word = to_word(probs, vocabularies)\n",
    "        #########################################\n",
    "        return poem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他的一些处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, poems_vec, word_to_int):\n",
    "    # 每次取64首诗进行训练\n",
    "    n_chunk = len(poems_vec) // batch_size\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "\n",
    "        batches = poems_vec[start_index:end_index]\n",
    "        # 找到这个batch的所有poem中最长的poem的长度\n",
    "        length = max(map(len, batches))\n",
    "        # 填充一个这么大小的空batch，空的地方放空格对应的index标号\n",
    "        x_data = np.full((batch_size, length), word_to_int[' '], np.int32)\n",
    "        for row in range(batch_size):\n",
    "            x_data[row, :len(batches[row])] = batches[row]\n",
    "        y_data = np.copy(x_data)\n",
    "        y_data[:, :-1] = x_data[:, 1:]\n",
    "        \"\"\"\n",
    "        x_data             y_data\n",
    "        [6,2,4,6,9]       [2,4,6,9,9]\n",
    "        [1,4,2,8,5]       [4,2,8,5,5]\n",
    "        \"\"\"\n",
    "        x_batches.append(x_data)\n",
    "        y_batches.append(y_data)\n",
    "    return x_batches, y_batches\n",
    "\n",
    "def to_word(predict, vocabs):# 预测的结果转化成汉字\n",
    "    sample = np.argmax(predict)\n",
    "    if sample > len(vocabs):\n",
    "        sample = len(vocabs) - 1\n",
    "    return vocabs[sample]\n",
    "def pretty_print_poem(poem):#  令打印的结果更工整\n",
    "    poem_sentences = poem.split('。')\n",
    "    for s in poem_sentences:\n",
    "        if s != '' and len(s) > 10:\n",
    "            print(s + '。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train tang poem...\n",
      "[INFO] Epoch: 0 , batch: 0 , training loss: 8.725339\n",
      "[INFO] Epoch: 0 , batch: 1 , training loss: 7.181267\n",
      "[INFO] Epoch: 0 , batch: 2 , training loss: 14.593356\n",
      "[INFO] Epoch: 0 , batch: 3 , training loss: 6.397608\n",
      "[INFO] Epoch: 0 , batch: 4 , training loss: 7.947126\n",
      "[INFO] Epoch: 0 , batch: 5 , training loss: 6.713815\n",
      "[INFO] Epoch: 0 , batch: 6 , training loss: 7.431762\n",
      "[INFO] Epoch: 0 , batch: 7 , training loss: 7.423326\n",
      "[INFO] Epoch: 0 , batch: 8 , training loss: 7.392948\n",
      "[INFO] Epoch: 0 , batch: 9 , training loss: 7.192723\n",
      "[INFO] Epoch: 0 , batch: 10 , training loss: 7.081545\n",
      "[INFO] Epoch: 0 , batch: 11 , training loss: 6.905571\n",
      "[INFO] Epoch: 0 , batch: 12 , training loss: 6.860982\n",
      "[INFO] Epoch: 0 , batch: 13 , training loss: 6.755879\n",
      "[INFO] Epoch: 0 , batch: 14 , training loss: 6.663974\n",
      "[INFO] Epoch: 0 , batch: 15 , training loss: 6.600811\n",
      "[INFO] Epoch: 0 , batch: 16 , training loss: 6.545947\n",
      "[INFO] Epoch: 0 , batch: 17 , training loss: 6.469064\n",
      "[INFO] Epoch: 0 , batch: 18 , training loss: 6.381902\n",
      "[INFO] Epoch: 0 , batch: 19 , training loss: 6.274572\n",
      "[INFO] Epoch: 0 , batch: 20 , training loss: 6.141372\n",
      "[INFO] Epoch: 0 , batch: 21 , training loss: 6.065341\n",
      "[INFO] Epoch: 0 , batch: 22 , training loss: 6.268845\n",
      "[INFO] Epoch: 0 , batch: 23 , training loss: 6.056138\n",
      "[INFO] Epoch: 0 , batch: 24 , training loss: 5.990842\n",
      "[INFO] Epoch: 0 , batch: 25 , training loss: 6.005237\n",
      "[INFO] Epoch: 0 , batch: 26 , training loss: 5.900696\n",
      "[INFO] Epoch: 0 , batch: 27 , training loss: 5.846792\n",
      "[INFO] Epoch: 0 , batch: 28 , training loss: 5.956637\n",
      "[INFO] Epoch: 0 , batch: 29 , training loss: 5.784215\n",
      "[INFO] Epoch: 0 , batch: 30 , training loss: 5.766388\n",
      "[INFO] Epoch: 0 , batch: 31 , training loss: 5.822543\n",
      "[INFO] Epoch: 0 , batch: 32 , training loss: 5.767713\n",
      "[INFO] Epoch: 0 , batch: 33 , training loss: 5.697567\n",
      "[INFO] Epoch: 0 , batch: 34 , training loss: 5.875203\n",
      "[INFO] Epoch: 0 , batch: 35 , training loss: 5.753194\n",
      "[INFO] Epoch: 0 , batch: 36 , training loss: 5.717970\n",
      "[INFO] Epoch: 0 , batch: 37 , training loss: 5.596324\n",
      "[INFO] Epoch: 0 , batch: 38 , training loss: 5.689659\n",
      "[INFO] Epoch: 0 , batch: 39 , training loss: 5.620527\n",
      "[INFO] Epoch: 0 , batch: 40 , training loss: 5.521164\n",
      "[INFO] Epoch: 0 , batch: 41 , training loss: 5.782165\n",
      "[INFO] Epoch: 0 , batch: 42 , training loss: 6.114949\n",
      "[INFO] Epoch: 0 , batch: 43 , training loss: 5.890157\n",
      "[INFO] Epoch: 0 , batch: 44 , training loss: 5.934635\n",
      "[INFO] Epoch: 0 , batch: 45 , training loss: 6.162814\n",
      "[INFO] Epoch: 0 , batch: 46 , training loss: 6.950754\n",
      "[INFO] Epoch: 0 , batch: 47 , training loss: 6.150836\n",
      "[INFO] Epoch: 0 , batch: 48 , training loss: 6.243834\n",
      "[INFO] Epoch: 0 , batch: 49 , training loss: 6.157350\n",
      "[INFO] Epoch: 0 , batch: 50 , training loss: 6.103909\n",
      "[INFO] Epoch: 0 , batch: 51 , training loss: 5.943595\n",
      "[INFO] Epoch: 0 , batch: 52 , training loss: 5.822884\n",
      "[INFO] Epoch: 0 , batch: 53 , training loss: 5.981018\n",
      "[INFO] Epoch: 0 , batch: 54 , training loss: 5.916040\n",
      "[INFO] Epoch: 0 , batch: 55 , training loss: 5.979182\n",
      "[INFO] Epoch: 0 , batch: 56 , training loss: 5.887392\n",
      "[INFO] Epoch: 0 , batch: 57 , training loss: 5.810349\n",
      "[INFO] Epoch: 0 , batch: 58 , training loss: 5.744154\n",
      "[INFO] Epoch: 0 , batch: 59 , training loss: 5.883266\n",
      "[INFO] Epoch: 0 , batch: 60 , training loss: 5.727438\n",
      "[INFO] Epoch: 0 , batch: 61 , training loss: 5.784498\n",
      "[INFO] Epoch: 0 , batch: 62 , training loss: 5.757965\n",
      "[INFO] Epoch: 0 , batch: 63 , training loss: 5.815999\n",
      "[INFO] Epoch: 0 , batch: 64 , training loss: 5.792866\n",
      "[INFO] Epoch: 0 , batch: 65 , training loss: 5.821064\n",
      "[INFO] Epoch: 0 , batch: 66 , training loss: 5.754052\n",
      "[INFO] Epoch: 0 , batch: 67 , training loss: 5.750280\n",
      "[INFO] Epoch: 0 , batch: 68 , training loss: 5.991974\n",
      "[INFO] Epoch: 0 , batch: 69 , training loss: 5.806551\n",
      "[INFO] Epoch: 0 , batch: 70 , training loss: 5.924456\n",
      "[INFO] Epoch: 0 , batch: 71 , training loss: 5.782222\n",
      "[INFO] Epoch: 0 , batch: 72 , training loss: 5.887060\n",
      "[INFO] Epoch: 0 , batch: 73 , training loss: 5.816978\n",
      "[INFO] Epoch: 0 , batch: 74 , training loss: 5.778757\n",
      "[INFO] Epoch: 0 , batch: 75 , training loss: 5.569281\n",
      "[INFO] Epoch: 0 , batch: 76 , training loss: 5.754959\n",
      "[INFO] Epoch: 0 , batch: 77 , training loss: 5.683517\n",
      "[INFO] Epoch: 0 , batch: 78 , training loss: 5.662950\n",
      "[INFO] Epoch: 0 , batch: 79 , training loss: 5.550289\n",
      "[INFO] Epoch: 0 , batch: 80 , training loss: 5.671493\n",
      "[INFO] Epoch: 0 , batch: 81 , training loss: 5.754161\n",
      "[INFO] Epoch: 0 , batch: 82 , training loss: 5.727168\n",
      "[INFO] Epoch: 0 , batch: 83 , training loss: 5.804455\n",
      "[INFO] Epoch: 0 , batch: 84 , training loss: 5.698023\n",
      "[INFO] Epoch: 0 , batch: 85 , training loss: 5.750699\n",
      "[INFO] Epoch: 0 , batch: 86 , training loss: 5.740923\n",
      "[INFO] Epoch: 0 , batch: 87 , training loss: 5.738308\n",
      "[INFO] Epoch: 0 , batch: 88 , training loss: 5.865658\n",
      "[INFO] Epoch: 0 , batch: 89 , training loss: 5.709999\n",
      "[INFO] Epoch: 0 , batch: 90 , training loss: 5.644366\n",
      "[INFO] Epoch: 0 , batch: 91 , training loss: 5.666833\n",
      "[INFO] Epoch: 0 , batch: 92 , training loss: 5.680334\n",
      "[INFO] Epoch: 0 , batch: 93 , training loss: 5.695117\n",
      "[INFO] Epoch: 0 , batch: 94 , training loss: 5.801347\n",
      "[INFO] Epoch: 0 , batch: 95 , training loss: 5.728564\n",
      "[INFO] Epoch: 0 , batch: 96 , training loss: 5.618693\n",
      "[INFO] Epoch: 0 , batch: 97 , training loss: 5.693699\n",
      "[INFO] Epoch: 0 , batch: 98 , training loss: 5.681196\n",
      "[INFO] Epoch: 0 , batch: 99 , training loss: 5.673175\n",
      "[INFO] Epoch: 0 , batch: 100 , training loss: 5.575947\n",
      "[INFO] Epoch: 0 , batch: 101 , training loss: 5.607211\n",
      "[INFO] Epoch: 0 , batch: 102 , training loss: 5.752081\n",
      "[INFO] Epoch: 0 , batch: 103 , training loss: 5.534345\n",
      "[INFO] Epoch: 0 , batch: 104 , training loss: 5.583841\n",
      "[INFO] Epoch: 0 , batch: 105 , training loss: 5.802829\n",
      "[INFO] Epoch: 0 , batch: 106 , training loss: 5.748958\n",
      "[INFO] Epoch: 0 , batch: 107 , training loss: 5.723555\n",
      "[INFO] Epoch: 0 , batch: 108 , training loss: 5.527332\n",
      "[INFO] Epoch: 0 , batch: 109 , training loss: 5.472649\n",
      "[INFO] Epoch: 0 , batch: 110 , training loss: 5.670778\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4a20036293e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[INFO] train tang poem...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;31m#print('[INFO] write tang poem...')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m#poem2 = gen_poem('月')# 生成诗歌\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m#print(\"#\" * 25)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c70b05b5502d>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mend_points\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'last_state'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mend_points\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_op'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 ], feed_dict={input_data: batches_inputs[n], output_targets: batches_outputs[n]})\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[INFO] Epoch: %d , batch: %d , training loss: %.6f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lamwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lamwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lamwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lamwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\lamwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('[INFO] train tang poem...')\n",
    "run_training() # 训练模型\n",
    "#print('[INFO] write tang poem...')\n",
    "#poem2 = gen_poem('月')# 生成诗歌\n",
    "#print(\"#\" * 25)\n",
    "#pretty_print_poem(poem2)\n",
    "#print('#' * 25)\n",
    "#训练模型时间比较长，训练模型完成后每次生成诗歌的时，不需要再次训练 ，可以注销上面的 run_training()。生成部分执行速度很快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
